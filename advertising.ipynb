{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db4e0766",
   "metadata": {},
   "source": [
    "# Optimal Advertising Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce28aaa",
   "metadata": {},
   "source": [
    "A company is considering whether to make an ad buy for one million dollars.  In a good year, the company has sales of 4 million dollars, in a normal year, its sales are 2 million dollars, and in a bad year, its sales are only 1 million dollars.  The company thinks has an internal rate of return on other projects at gross rate 1+r\n",
    "\n",
    "We use matrices to represent the impact of advertising; the entry in row i, column j, represents the probability of entering state j when one is currently in state i. Here, we let the first row represents a bad year, the second a normal year, and the third a good year. The columns are likewise. This would imply that the top right entry is the probability of having a bad year next year, when one has just had a bad year. \n",
    "\n",
    "If there is no ad buy, the Markov matrix describing transitions between states is:\n",
    "\n",
    "$$\n",
    "M(0) = \\left[\\begin{array}{ccc}\n",
    "0.8 & 0.2 & 0 \\\\\n",
    "0.2 & 0.8 & 0 \\\\\n",
    "0.1 & 0.4 & 0.5 \\\\\n",
    "\\end{array} \\right]\n",
    "$$\n",
    "\n",
    "If there is an ad buy, the Markov matrix describing transitions between states is:\n",
    "\n",
    "$$\n",
    "M(1) = \\left[\\begin{array}{ccc}\n",
    "0.5 & 0.5 & 0 \\\\\n",
    "0.1 & 0.6 & 0.3 \\\\\n",
    "0.1 & 0.3 & 0.6 \\\\\n",
    "\\end{array} \\right]\n",
    "$$\n",
    "\n",
    "As we can see, advertising significantly improves the expectation of next years sales in all three states. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ee6422",
   "metadata": {},
   "source": [
    "# Bellman Equations\n",
    "\n",
    "We use Bellman equations to evaluate the firm's optimal strategy in the dynamic program. A dynamic program is a set of uncertain outcomes, strategies, and rewards. Our solution will be a set of optimal strategies for all states. For example, in this case our solution could be that the firm should always invest in advertising in good or normal years, and cut the advertising budget in bad years. \n",
    "\n",
    "The dynamic program is defined as follows;\n",
    "\n",
    "1. The state space is $X = \\{Bad, Normal, Good\\}$\n",
    "2. The control set is $U = \\{0, 1\\}$, where 0 means no ad campaign and 1 means make an ad buy.\n",
    "3. The reward function $r(x,u)$ is given by these six numbers:\n",
    "\n",
    "$$\n",
    "r(Bad, 0) = 1 \\\\\n",
    "r(Bad, 1) = 0 \\\\\n",
    "r(Normal, 0) = 2 \\\\\n",
    "r(Normal, 1) = 1 \\\\\n",
    "r(Good, 0) = 4 \\\\\n",
    "r(Good, 1) = 3\\\\\n",
    "$$\n",
    "\n",
    "4. The transition rule is described by the two Markov matrices $M(0)$ and $M(1)$.\n",
    "5. The discount factor for this problem is $\\beta = 1/(1+r)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e542856f",
   "metadata": {},
   "source": [
    "Which leads to the following Bellman equation, whose value we will seek to maximize;\n",
    "\n",
    "$$\n",
    "V(Normal) = max \\left\\{ 2 + \\beta [0.2 V(Bad) + 0.8 V(Normal)], 1 + \\beta [0.1 V(Bad) + 0.6 V(Normal)+ 0.3 V(Good)]\\right\\}\n",
    "$$\n",
    "\n",
    "The Bellman equation represents the certain reward for the decision in the current state, plus the expected value of the decision in the future state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb8263f",
   "metadata": {},
   "source": [
    "We define our paramaters below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d8953ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #271: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import quantecon as qe\n",
    "\n",
    "States=['Bad', 'Normal', 'Good']\n",
    "Controls=['No buy','Buy']\n",
    "reward=np.array([[1,0],[2,1],[4,3]])\n",
    "beta=0.9\n",
    "value=np.zeros(len(States))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4de5e3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "M0=np.array([[0.8,0.2,0],[0.2,0.8,0],[0.1,0.4,0.5]])\n",
    "M1=np.array([[0.5,0.5,0],[0.1,0.6,0.3],[0.1,0.3,0.6]])\n",
    "markov_matrices=[M0,M1]\n",
    "transition={} #empty dictionary\n",
    "for control in Controls:\n",
    "    transition[control]=markov_matrices[Controls.index(control)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49bacc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_parameters(States,Controls,reward,transition,value,beta):\n",
    "    \"\"\"\n",
    "    The value function has n elements\n",
    "    The control set has m elements\n",
    "    The reward function is an n x m matrix\n",
    "    The transition is a list of m Markov matrices, each with dimesnion n x n\n",
    "    The discount factor must satisfy 0 <= beta < 1\n",
    "    \"\"\"\n",
    "    n=len(States)\n",
    "    m=len(Controls)\n",
    "    if len(value) != n: print(\"The value function and the state space are not conformable.\")\n",
    "    if reward.shape != (n,m): print(\"The reward function is not well defined.\")\n",
    "    if len(transition) != m: \n",
    "            print(\"There is not the requisite number of transition matrices.\")\n",
    "    if not all(transition[control].shape == (n,n) for control in Controls):\n",
    "        print(\"At least one of the Markov transitions is of the wrong dimension.\")\n",
    "    if not all(np.ones(n)@transition[control]@np.ones(n) == n for control in Controls):\n",
    "        print(\"Give me a break! One of the transition matrices is not Markov.\")\n",
    "    if not all((transition[control]>=0).all() and (transition[control]<=1).all() for control in Controls):\n",
    "            print(\"Give me a break! The elements of the transition matrices are not probabilities.\")   \n",
    "    if beta < 0  or beta >= 1:\n",
    "        print(\"Something is wrong with the discount factor.\")\n",
    "    return (print(\"I have done the requisite checks.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c1c35ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_state_probs(state,control,transition):\n",
    "    \"\"\"\n",
    "    This function takes the state set, the control set, and the list of transition matrices\n",
    "    It returns the relevant row of the transition matrix in state s, when control u is chosen\n",
    "    \"\"\"\n",
    "    return(transition[control][States.index(state)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2735532a",
   "metadata": {},
   "source": [
    "# The Howard Method\n",
    "\n",
    "To discover the optimal set of strategies, we use the the Howard Method. The algorithm has the following steps;\n",
    "\n",
    "1. Make a guess for the optimal set of strategies\n",
    "\n",
    "2. See if these strategies maximize the reward function using the Bellman Equation\n",
    "\n",
    "3. If they do, then the solution is found, if else, repeat with the set of strategies that was higher.\n",
    "\n",
    "We define this algorithm below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "708df69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_value(States, Controls, reward, transition, value, beta):\n",
    "    \"\"\"\n",
    "    The value function has n elements\n",
    "    The control set has m elements\n",
    "    The reward function is an n x m matrix\n",
    "    The transition is a list of m Markov matrices, each with dimesnion n x n\n",
    "    The discount factor must satisfy 0 <= beta < 1\n",
    "    \"\"\"\n",
    "    v_next=[]\n",
    "    for state in States:\n",
    "        bellman=[]\n",
    "        for control in Controls:\n",
    "            bellman.append(reward[States.index(state),\n",
    "        Controls.index(control)]+beta*(value@next_state_probs(state,control,transition)))\n",
    "        v_next.append(max(bellman))\n",
    "    return(v_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee5f26c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=[1.1,1.2,-3,5]\n",
    "X.index(min(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb7ea592",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve(States, Controls, reward, transition, value, beta):\n",
    "    check=1.0 #check for convergence\n",
    "    epsilon=10e-5 #epsilon to define convergence\n",
    "    max_iter = 100 #maximum iterations for the while loop\n",
    "    iter=0 #count iterations\n",
    "    while check > epsilon:\n",
    "        v1 = np.array(next_value(States,Controls,reward,transition,value,beta))\n",
    "        check=max(abs(value-v1))\n",
    "        value = v1 #reset the value function\n",
    "        iter += 1 #advance the counter\n",
    "        if(iter > max_iter): #get out of Dodge, if we are in an infinite loop\n",
    "            print(\"Algorithm did not converge\")\n",
    "            break\n",
    "    return(value) \n",
    "\n",
    "value=solve(States,Controls,reward,transition,value,beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1e1d656",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_policy(States,Controls,reward,transition,value,beta):\n",
    "    policy=[]\n",
    "    for state in States:\n",
    "        z=[]\n",
    "        for control in Controls:\n",
    "            z.append(reward[States.index(state),Controls.index(control)]+beta*(value@next_state_probs(state,control,transition)))\n",
    "        policy.append(Controls[z.index(max(z))])\n",
    "    return(policy)\n",
    "\n",
    "policy=optimal_policy(States,Controls,reward,transition,value,beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f3955cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def M_optimal(States,transition,policy):\n",
    "    M=np.empty_like(list(transition.items())[0][1])\n",
    "    for state in States:\n",
    "        M[States.index(state),:]=transition[policy[States.index(state)]][States.index(state),:]\n",
    "    return(M)\n",
    "\n",
    "P=M_optimal(States,transition,policy)\n",
    "from quantecon import MarkovChain\n",
    "mc = qe.MarkovChain(P)\n",
    "p_stationary=mc.stationary_distributions  # Show all stationary distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a68d41ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No buy</th>\n",
       "      <th>Buy</th>\n",
       "      <th>value function</th>\n",
       "      <th>optimal policy</th>\n",
       "      <th>probabilities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Bad</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>14.804697</td>\n",
       "      <td>No buy</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Normal</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>17.474440</td>\n",
       "      <td>Buy</td>\n",
       "      <td>0.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Good</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>21.132976</td>\n",
       "      <td>No buy</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        No buy  Buy  value function optimal policy  probabilities\n",
       "Bad          1    0       14.804697         No buy       0.333333\n",
       "Normal       2    1       17.474440            Buy       0.416667\n",
       "Good         4    3       21.132976         No buy       0.250000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PS5=pd.DataFrame(reward, States, Controls)\n",
    "PS5[\"value function\"]=value\n",
    "PS5[\"optimal policy\"]=policy\n",
    "PS5[\"probabilities\"]=p_stationary[0] #only one distribution in the list that was returned\n",
    "PS5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
